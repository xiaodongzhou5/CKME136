---
title: "capstone_xz_ml"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
install.packages('dlookr')
library(dlookr)
install.packages('RCurl')
install.packages('MASS')
install.packages('leaps')
library(RCurl) # getURL 
library(MASS) # stepwise regression
library(leaps) # all subsets regression
```

# read *norm file, 
```{r}
df_norm <- read.csv(file ='c:/work/source/cell5m_17_na_rm_norm.csv',stringsAsFactors = TRUE)
str(df_norm)
summary(df_norm)


```


## subset training data for multiple linear regression

```{r}
data_train <- sample(nrow(df_norm), floor(nrow(df_norm)*0.7))
train <- df_norm[data_train,]
test <- df_norm[-data_train,]
```

multiple linear regression model to predict 'vp' variable. 
```{r}
model_mlr_norm <- lm(vp_cr_ar_r~pre_mean+LGP_AVG+ELEVATION+SLOPE+soc_d5+soc_d60+soc_d200+PN05_TOT+PN05_RUR+TT_PORT+TT_20K, data=train) 
prediction_norm <- predict(model_mlr_norm, interval="prediction", newdata =test)
```

#plot error on a histogram. 
```{r}
errors_norm <- prediction_norm[,"fit"] - test$vp_cr_ar_r
hist(errors_norm)
```
#note that the residual error shown above is not normally distributed.  
#log+1 and sqrt transformed data however, results in normal redisual error distribution

#Compute the root mean square error and find the percentage of cases with less than 25% error.
```{r}
rmse <- sqrt(sum((prediction_norm[,"fit"] - test$vp_cr_ar_r)^2)/nrow(test))
rel_change <- 1 - ((test$vp_cr_ar_r - abs(errors)) / test$vp_cr_ar_r)
pred25 <- table(rel_change<0.25)["TRUE"] / nrow(test)
paste("RMSE:", rmse)
paste("PRED(25):", pred25)
```

simple linear regression model. Compare the results with the multiple linear regression.
```{r}
rn_train <- sample(nrow(df_norm), floor(nrow(df_norm)*0.7))
train <- df_norm[rn_train,c("vp_cr_ar_r","soc_d5")]
test <- df_norm[-rn_train,c("vp_cr_ar_r","soc_d5")]
model_ulm <- lm(vp_cr_ar_r~soc_d5, data=train) 
prediction <- predict(model_ulm, interval="prediction", newdata =test)
errors <- prediction[,"fit"] - test$vp_cr_ar_r
hist(errors)
rmse <- sqrt(sum((prediction[,"fit"] - test$vp_cr_ar_r)^2)/nrow(test))
rel_change <- 1 - ((test$vp_cr_ar_r - abs(errors)) / test$vp_cr_ar_r)
pred25 <- table(rel_change<0.25)["TRUE"] / nrow(test)
paste("RMSE:", rmse)
paste("PRED(25):", pred25)
```

#Pred(25) is better for multiple linear regression, RMSE is similar for both.
#Multiple linear regression needs twigging by feature selection.

#Forward selection algorithm. start with 'null', to come up with a selection of independent variables between 'null' and 'full'. 
```{r echo=TRUE}
library(MASS)
full_norm <- lm(vp_cr_ar_r~pre_mean+LGP_AVG+ELEVATION+SLOPE+soc_d5+soc_d60+soc_d200+PN05_TOT+PN05_RUR+TT_PORT+TT_20K, data=df_norm)
null_norm <- lm(vp_cr_ar_r~1,data=df_norm)
stepF_norm <- stepAIC(null_norm, scope=list(lower=null_norm, upper=full_norm), direction= "forward", trace=TRUE)
summary(stepF_norm)
```
#Lowest AIC is to use all variables.  trace = T to see all steps

#'backward' elimination, which will start with 'full'.
```{r echo=TRUE}
full_norm <- lm(vp_cr_ar_r~pre_mean+LGP_AVG+ELEVATION+SLOPE+soc_d5+soc_d60+soc_d200+PN05_TOT+PN05_RUR+TT_PORT+TT_20K,data=df_norm)
stepB_norm <- stepAIC(full_norm, direction= "backward", trace=TRUE)
summary(stepB_norm)
```
#Use all the variables except "slope" (p > 0.05).

#test the best combination of the 12 attributes.
```{r}
library(leaps)
subsets<-regsubsets(vp_cr_ar_r~pre_mean+LGP_AVG+ELEVATION+SLOPE+soc_d5+soc_d60+soc_d200+PN05_TOT+PN05_RUR+TT_PORT+TT_20K, data = df_norm, nbest=1,)
sub.sum <- summary(subsets)
as.data.frame(sub.sum$outmat)


```
In the output * denotes the included variables.
The best combination of 8 attributes is: 'pre_mean', 'LGP_AVG', 'ELEVATION', 'soc_d5', 'PN05_TOT' and 'PN05_RUR', 'TT_PORT' and 'TT_20K'.
The best combination of 5 attributes is: 'pre_mean', 'LGP_AVG', 'ELEVATION', 'PN05_RUR', 'TT_PORT'.







Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

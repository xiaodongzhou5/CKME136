---
title: "capstone_xz_expln_decisionTr"
output: word_document
---


```{r cars}
install.packages('party')
library(party)
```


read data set 
```{r}
df_norm_bin <- read.csv(file ='c:/work/source/cell5m_17_na_rm_norm_binnedTarget.csv', stringsAsFactors = TRUE)
df_norm_bin$vp_n_bin <- factor(df_norm_bin$vp_n_bin, levels = c('[0,0.0211]', '(0.0211,0.0376]', '(0.0376,0.0607]', '(0.0607,1]'))
str(df_norm_bin)
```

Using the 'ctree' function: 1) model 1 with all 13 attributes 2) exclude factor attributes
```{r}
library(party)
data_ctree1 <- ctree(vp_n_bin ~  pre_mean + LGP_AVG + ELEVATION + SLOPE + soc_d5 + soc_d60 + soc_d200 + PN05_TOT + PN05_RUR + TT_PORT + TT_20K + AEZ8_CLAS + FS_2012_TX, data = df_norm_bin)

data_ctree2 <- ctree(vp_n_bin ~  pre_mean + LGP_AVG + ELEVATION + SLOPE + soc_d5 + soc_d60 + soc_d200 + PN05_TOT + PN05_RUR + TT_PORT + TT_20K, data = df_norm_bin)
```

print the decision tree.
```{r}
print(data_ctree1)
print(data_ctree2)
```

plot the decision tree.
```{r}
plot(data_ctree1, type="simple")
plot(data_ctree2, type="simple")
```

Use decision trees as classifiers. split the dataset into training and test set.
```{r}
train_index1 <- sample(1:nrow(df_norm_bin), 0.7 * nrow(df_norm_bin))
train.set1 <- df_norm_bin[train_index1,]
test.set1  <- df_norm_bin[-train_index1,]
```

run model on the training set, use all 13 independent variables
```{r}
str(df_norm_bin)
data_ctree_model1 <- ctree(vp_n_bin ~  pre_mean + LGP_AVG + ELEVATION + SLOPE + soc_d5 + soc_d60 + soc_d200 + PN05_TOT + PN05_RUR + TT_PORT + TT_20K + AEZ8_CLAS + FS_2012_TX, data=train.set1)
data_ctree_model1
```
top 8 attributes with high entropy (or information gain): FX_2012_TX, AEZ8_CLAS, LGP_AVG, TT_PORT, pre_mean, ELEVATION, soc_d200, PN05_RUR, TT_20K, soc_d60

make prediction on the test set.
```{r}
data_ctree_prediction1 <- predict(data_ctree_model1, test.set1) 
# gives the probability for each class
head(data_ctree_prediction1)
```

review the confusion matrix.
```{r}
table(data_ctree_prediction1, test.set1$vp_n_bin)
```

Using the 'ctree' function, model 2 with selected 11 features (excluded LGP_AVG with 0.87 coeff with pre_mean, and soc_60 with 0.86 with soc_05)
```{r}
library(party)

memory.size(max = TRUE)
memory.limit(size = 45000)

data_ctree_model2 <- ctree(vp_n_bin ~  pre_mean + ELEVATION + SLOPE + soc_d5 + soc_d200 + PN05_TOT + PN05_RUR + TT_PORT + TT_20K + AEZ8_CLAS + FS_2012_TX, data=train.set1)
data_ctree_model2

```
make prediction on the test set.
```{r}
data_ctree_prediction2 <- predict(data_ctree_model2, test.set1) 
# gives the probability for each class
head(data_ctree_prediction2)
```
review the confusion matrix.
```{r}
table(data_ctree_prediction2, test.set1$vp_n_bin)
```
#Model 2 yields slightly lower accuracy than that of model 1.

#feature engineering
```{r}
df_norm_bin$soc_avg <- c((df_norm_bin$soc_d5+df_norm_bin$soc_d60+df_norm_bin$soc_d200)/3)
df_norm_bin$TT_avg <- c((df_norm_bin$TT_PORT+df_norm_bin$TT_20K)/2)
df_norm_bin$TT_min <- with(df_norm_bin, ifelse(df_norm_bin$TT_PORT <= df_norm_bin$TT_20K, df_norm_bin$TT_PORT, df_norm_bin$TT_20K))
summary(df_norm_bin)
str(df_norm_bin)
```


#Using the 'ctree' function, model 3 with selected 7 features (including 2 engineered features, excluded LGP_AVG with 0.87 coeff with pre_mean)
```{r}
library(party)

memory.size(max = TRUE)
memory.limit(size = 45000)

data_ctree3 <- ctree(vp_n_bin ~  pre_mean + ELEVATION + soc_avg + PN05_RUR + TT_min + AEZ8_CLAS + FS_2012_TX, data = df_norm_bin)
 

```

#print the decision tree.
```{r}
print(data_ctree3)

```

#plot the decision tree.
```{r}
plot(data_ctree3, type="simple")

```

#run model on the training set.
```{r}
str(df_norm_bin)
data_ctree_model3 <- ctree(vp_n_bin ~  pre_mean + ELEVATION + soc_avg + PN05_RUR + TT_min + AEZ8_CLAS + FS_2012_TX, data=train.set1)
data_ctree_model3
```

#make prediction on the test set.
```{r}
data_ctree_prediction3 <- predict(data_ctree_model3, test.set1) 
# gives the probability for each class
head(data_ctree_prediction3)
```

#review the confusion matrix.
```{r}
table(data_ctree_prediction3, test.set1$vp_n_bin)
```
#model 3 yields slightly higher accuracy than other ctree models.  It requires only 7 predicator attributes, instead of 13 and is considered best ctree model.

